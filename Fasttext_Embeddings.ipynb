{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id keyword location                                               text\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...\n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada\n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...\n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...\n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...\n",
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "\n",
    "DATA_DIR = 'data/'\n",
    "\n",
    "train_file = DATA_DIR + 'train.csv'\n",
    "test_file = DATA_DIR + 'test.csv'\n",
    "\n",
    "\n",
    "def convert_to_numpy(df):\n",
    "    print(df)\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    return pd.read_csv(test_file, header=0)\n",
    "\n",
    "def load_train_data():\n",
    "    df = pd.read_csv(train_file, header=0)\n",
    "    return df.iloc[:,:-1], df.iloc[:,-1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataX, datay = load_train_data()\n",
    "    print(dataX.head())\n",
    "    print(datay.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile url regex (a basic one)\n",
    "url_regex = re.compile(\"(http|https)://[A-Za-z0-9_-]+\\.[A-Za-z0-9_-]+(/\\S*)?\")\n",
    "# Compile regex to detect tokens that are entirely non-text\n",
    "nontext_regex = re.compile(\"[^A-Za-z]+\")\n",
    "# Compile regex to detect @ mentions\n",
    "mention_regex = re.compile(\"@\\S+\")\n",
    "# Compile regex to detect various mis-encoded punctuation characters\n",
    "misenc_regex = re.compile(\"&amp;\")\n",
    "# Compile regex to check if text is composed entirely of letters and digits\n",
    "alphanum_regex = re.compile(\"[A-Za-z0-9]+\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Our Deeds are the Reason of this #earthquake M...\n",
       "1                  Forest fire near La Ronge Sask. Canada\n",
       "2       All residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       Just got sent this photo from Ruby #Alaska as ...\n",
       "5       #RockyFire Update => California Hwy. 20 closed...\n",
       "6       #flood #disaster Heavy rain causes flash flood...\n",
       "7       I'm on top of the hill and I can see a fire in...\n",
       "8       There's an emergency evacuation happening now ...\n",
       "9       I'm afraid that the tornado is coming to our a...\n",
       "10            Three people died from the heat wave so far\n",
       "11      Haha South Tampa is getting flooded hah- WAIT ...\n",
       "12      #raining #flooding #Florida #TampaBay #Tampa 1...\n",
       "13                #Flood in Bago Myanmar #We arrived Bago\n",
       "14      Damage to school bus on 80 in multi car crash ...\n",
       "15                                         What's up man?\n",
       "16                                          I love fruits\n",
       "17                                       Summer is lovely\n",
       "18                                      My car is so fast\n",
       "19                           What a goooooooaaaaaal!!!!!!\n",
       "20                                 this is ridiculous....\n",
       "21                                      London is cool ;)\n",
       "22                                            Love skiing\n",
       "23                                  What a wonderful day!\n",
       "24                                               LOOOOOOL\n",
       "25                         No way...I can't eat that shit\n",
       "26                                  Was in NYC last week!\n",
       "27                                     Love my girlfriend\n",
       "28                                              Cooool :)\n",
       "29                                     Do you like pasta?\n",
       "                              ...                        \n",
       "7583    Pic of 16yr old PKK suicide bomber who detonat...\n",
       "7584    These boxes are ready to explode! Exploding Ki...\n",
       "7585    Calgary Police Flood Road Closures in Calgary....\n",
       "7586    #Sismo DETECTADO #JapÌ_n 15:41:07 Seismic inte...\n",
       "7587                                   Sirens everywhere!\n",
       "7588    BREAKING: #ISIS claims responsibility for mosq...\n",
       "7589                                       Omg earthquake\n",
       "7590    SEVERE WEATHER BULLETIN No. 5 FOR: TYPHOON ÛÏ...\n",
       "7591    Heat wave warning aa? Ayyo dei. Just when I pl...\n",
       "7592    An IS group suicide bomber detonated an explos...\n",
       "7593    I just heard a really loud bang and everyone i...\n",
       "7594    A gas thing just exploded and I heard screams ...\n",
       "7595    NWS: Flash Flood Warning Continued for Shelby ...\n",
       "7596    RT @LivingSafely: #NWS issues Severe #Thunders...\n",
       "7597    #??? #?? #??? #??? MH370: Aircraft debris foun...\n",
       "7598    Father-of-three Lost Control of Car After Over...\n",
       "7599    1.3 #Earthquake in 9Km Ssw Of Anza California ...\n",
       "7600    Evacuation order lifted for town of Roosevelt:...\n",
       "7601    #breaking #LA Refugio oil spill may have been ...\n",
       "7602    a siren just went off and it wasn't the Forney...\n",
       "7603    Officials say a quarantine is in place at an A...\n",
       "7604    #WorldNews Fallen powerlines on G:link tram: U...\n",
       "7605    on the flip side I'm at Walmart and there is a...\n",
       "7606    Suicide bomber kills 15 in Saudi security site...\n",
       "7607    #stormchase Violent Record Breaking EF-5 El Re...\n",
       "7608    Two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @TheTawniest The out of control w...\n",
       "7610    M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...\n",
       "7611    Police investigating after an e-bike collided ...\n",
       "7612    The Latest: More Homes Razed by Northern Calif...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataX['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "['Our', 'Deeds', 'Reason', 'earthquake', 'May', 'ALLAH', 'Forgive']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i in dataX[\\'text\\']:\\n    text = url_regex.sub(\"\", text)\\n    text = misenc_regex.sub(\"\", text)\\n    text = mention_regex.sub(\"\", text)\\n    text = re.sub(\"#\", \"\", text)\\n    text = remove_stopwords(text)\\n    words = text.split(\" \")\\n    \\nprint(words)    \\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataX['text'][0]\n",
    "print(text)\n",
    "text = url_regex.sub(\"\", text)\n",
    "text = misenc_regex.sub(\"\", text)\n",
    "text = mention_regex.sub(\"\", text)\n",
    "text = re.sub(\"#\", \"\", text)\n",
    "text = remove_stopwords(text)\n",
    "text = strip_numeric(text)\n",
    "text = strip_punctuation(text)\n",
    "text = strip_multiple_whitespaces(text)\n",
    "words = text.split(\" \")\n",
    "print(words)\n",
    "\n",
    "'''\n",
    "for i in dataX['text']:\n",
    "    text = url_regex.sub(\"\", text)\n",
    "    text = misenc_regex.sub(\"\", text)\n",
    "    text = mention_regex.sub(\"\", text)\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = remove_stopwords(text)\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "print(words)    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = []\n",
    "#for i in dataX['text'][0:10]:\n",
    "for i in dataX['text']:\n",
    "    text = url_regex.sub(\"\", i)\n",
    "    text = misenc_regex.sub(\"\", text)\n",
    "    text = mention_regex.sub(\"\", text)\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = strip_numeric(text)\n",
    "    text = strip_punctuation(text)\n",
    "    text = strip_multiple_whitespaces(text)\n",
    "    words = text.split(\" \")\n",
    "    sentence.append(words)\n",
    "#print(sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.01575068,  0.05351213, -0.01825128,  0.01451734],\n",
      "       [ 0.0185172 ,  0.03048961, -0.00839526,  0.02434803],\n",
      "       [-0.01146877,  0.05369632, -0.04157733, -0.0052736 ]],\n",
      "      dtype=float32), array([[-0.03524481,  0.03532043,  0.04959469, -0.02352963],\n",
      "       [-0.03324692, -0.06451701, -0.01015481, -0.04317482],\n",
      "       [-0.01146877,  0.05369632, -0.04157733, -0.0052736 ],\n",
      "       [-0.03050808, -0.00921003,  0.0233555 ,  0.00851412],\n",
      "       [ 0.02457302, -0.05300654, -0.00205131,  0.00771587],\n",
      "       [-0.01400802,  0.04563731,  0.0210558 ,  0.02731517]],\n",
      "      dtype=float32), array([[ 0.13908893,  0.0225455 ,  0.02700579,  0.01246016],\n",
      "       [-0.03324692, -0.06451701, -0.01015481, -0.04317482],\n",
      "       [ 0.0185172 ,  0.03048961, -0.00839526,  0.02434803],\n",
      "       [-0.03050808, -0.00921003,  0.0233555 ,  0.00851412]],\n",
      "      dtype=float32), array([[-0.03050808, -0.00921003,  0.0233555 ,  0.00851412],\n",
      "       [ 0.01575068,  0.05351213, -0.01825128,  0.01451734],\n",
      "       [-0.03050808, -0.00921003,  0.0233555 ,  0.00851412],\n",
      "       [ 0.13908893,  0.0225455 ,  0.02700579,  0.01246016]],\n",
      "      dtype=float32), array([[-0.03324692, -0.06451701, -0.01015481, -0.04317482],\n",
      "       [ 0.02457302, -0.05300654, -0.00205131,  0.00771587],\n",
      "       [-0.01400802,  0.04563731,  0.0210558 ,  0.02731517]],\n",
      "      dtype=float32), array([[-0.01595963, -0.05089292, -0.03603407, -0.00170566]],\n",
      "      dtype=float32), array([[-0.00228575,  0.08778233,  0.0271456 ,  0.04995923],\n",
      "       [-0.01595963, -0.05089292, -0.03603407, -0.00170566]],\n",
      "      dtype=float32), array([[-0.00228575,  0.08778233,  0.0271456 ,  0.04995923],\n",
      "       [-0.01261545,  0.00968695,  0.03232732, -0.0292588 ],\n",
      "       [-0.01595963, -0.05089292, -0.03603407, -0.00170566]],\n",
      "      dtype=float32), array([[-0.00228575,  0.08778233,  0.0271456 ,  0.04995923],\n",
      "       [-0.01261545,  0.00968695,  0.03232732, -0.0292588 ],\n",
      "       [-0.03524481,  0.03532043,  0.04959469, -0.02352963]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "# use example set of texts\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "#build the model and train\n",
    "#use vector size 4 for the example\n",
    "model = FastText(size=4, window=3, min_count=1)  # instantiate\n",
    "model.build_vocab(sentences=common_texts)\n",
    "model.train(sentences=common_texts, total_examples=len(common_texts), epochs=10)  # train\n",
    "\n",
    "model.wv['computer']\n",
    "\n",
    "#produce a list of word embeddings for the example\n",
    "veclist = []\n",
    "for i in common_texts[0:len(common_texts)]:\n",
    "    veclist.append(model.wv[i])\n",
    "print(veclist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "#build the model and train\n",
    "#use vector size 4 for the example\n",
    "model1 = FastText(size=4, window=3, min_count=1)  # instantiate\n",
    "#model1.build_vocab(sentences=words)\n",
    "#model1.train(sentences=words, total_examples=1, epochs=10)  # train\n",
    "model1.build_vocab(sentences=sentence)\n",
    "model1.train(sentences=sentence, total_examples=len(sentence), epochs=10)  # train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.705604  ,  2.8683395 , -0.6167746 , -0.48912165], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv['Deeds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#produce a list of word embeddings for the example\n",
    "veclist = []\n",
    "for i in sentence[0:len(sentence)]:\n",
    "    veclist.append(model.wv[i])\n",
    "#print(veclist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.502542  ,  5.522774  , -0.88677776, -1.04765   ],\n",
       "       [-1.1531698 ,  6.3356037 , -0.98533607, -0.9949615 ],\n",
       "       [-0.8509851 ,  3.3678527 , -0.64768577, -0.5613398 ],\n",
       "       [-0.76767725,  2.8297114 , -0.43349996, -0.4145956 ],\n",
       "       [-0.31355256,  1.6160703 , -0.32882726, -0.23860975],\n",
       "       [-0.57533723,  2.112945  , -0.36981273, -0.30562758]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv[sentence[0]]\n",
    "model1.wv[sentence[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n"
     ]
    }
   ],
   "source": [
    "print(len(veclist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
